{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Handwritten Digit Recognition with MNIST (Kaggle Digit Recognizer)\n\nIn this notebook, I aim to build a clear, step-by-step pipeline to classify handwritten digits (0–9) using the Kaggle Digit Recognizer dataset. I’ll use TensorFlow/Keras for model building and training, and I’ll interleave **first-person comments** in the code to explain what I’m doing at each step. Additionally, I will include **four data visualizations** to share insights gathered during exploratory data analysis and training.\nScore: 0.98932","metadata":{}},{"cell_type":"code","source":"# I start by importing all the libraries I need for data handling, visualization, and model building.\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Optional: Disable GPU to avoid CUDA errors\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:11:36.462659Z","iopub.execute_input":"2025-06-06T04:11:36.463051Z","iopub.status.idle":"2025-06-06T04:11:36.468613Z","shell.execute_reply.started":"2025-06-06T04:11:36.463028Z","shell.execute_reply":"2025-06-06T04:11:36.467755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1. Data Loading & Preprocessing","metadata":{}},{"cell_type":"code","source":"# I locate the CSV files in the Kaggle input directory and read them into pandas DataFrames.\ntrain_df = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntest_df  = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n\n# I separate the labels from the training features.\ny = train_df['label'].values\nX = train_df.drop(columns=['label']).values\n\n# I convert pixel values to floats in [0, 1], then reshape to (num_samples, 28, 28, 1).\nX = X.astype(np.float32) / 255.0\nX = X.reshape(-1, 28, 28, 1)\n\n# I do the same normalization and reshaping for the test set.\nX_test = test_df.values.astype(np.float32) / 255.0\nX_test = X_test.reshape(-1, 28, 28, 1)\n\n# I hold out 10% of the training set as a validation set for monitoring performance.\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.10, random_state=42, stratify=y\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:11:41.968941Z","iopub.execute_input":"2025-06-06T04:11:41.969332Z","iopub.status.idle":"2025-06-06T04:11:46.266441Z","shell.execute_reply.started":"2025-06-06T04:11:41.969305Z","shell.execute_reply":"2025-06-06T04:11:46.265509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rationale\n\n- I normalize pixel values to the range [0, 1] so that gradient-based optimizers converge more smoothly.\n- Reshaping to `(28, 28, 1)` is necessary because convolutional layers expect 2D inputs with a channel dimension.\n- Splitting off 10% as a validation set helps me track whether the model is overfitting during training.","metadata":{}},{"cell_type":"markdown","source":"### 2. Exploratory Data Analysis (EDA)\n2.1. Sample Images from Each Class","metadata":{}},{"cell_type":"code","source":"# I want to visualize one digit example for each class (0–9).\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfig.suptitle('Sample Handwritten Digits (0–9)', fontsize=16)\n\nfor digit in range(10):\n    # I find the first occurrence of `digit` in the training labels.\n    idx = np.where(y_train == digit)[0][0]\n    img = X_train[idx].reshape(28, 28)\n    ax = axes[digit // 5, digit % 5]\n    \n    ax.imshow(img, cmap='gray')\n    ax.set_title(f\"Label = {digit}\")\n    ax.axis('off')\n\nplt.tight_layout(rect=[0, 0, 1, 0.9])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:11:59.175385Z","iopub.execute_input":"2025-06-06T04:11:59.175710Z","iopub.status.idle":"2025-06-06T04:12:00.140111Z","shell.execute_reply.started":"2025-06-06T04:11:59.175677Z","shell.execute_reply":"2025-06-06T04:12:00.139041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# I compute how many samples of each digit appear in the training set.\nlabel_counts = pd.Series(y_train).value_counts().sort_index()\n\n# I plot a bar chart to show the class balance.\nplt.figure(figsize=(8, 4))\nplt.bar(label_counts.index, label_counts.values, color='skyblue')\nplt.title('Training Set Label Distribution')\nplt.xlabel('Digit Label')\nplt.ylabel('Count')\nplt.xticks(range(10))\nplt.grid(axis='y', linestyle='--', alpha=0.7)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:12:10.434594Z","iopub.execute_input":"2025-06-06T04:12:10.434938Z","iopub.status.idle":"2025-06-06T04:12:10.655084Z","shell.execute_reply.started":"2025-06-06T04:12:10.434913Z","shell.execute_reply":"2025-06-06T04:12:10.654051Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Rationale\n\n- A roughly uniform distribution across classes (0–9) indicates balanced data.\n- If I saw a severe imbalance, I might need to consider class weighting or data augmentation specifically for underrepresented classes.\n","metadata":{}},{"cell_type":"markdown","source":"### 3. Model Definition","metadata":{}},{"cell_type":"code","source":"# I build a straightforward Convolutional Neural Network (CNN) for classification.\ndef create_cnn_model():\n    model = Sequential([\n        # First Convolutional Block\n        Input(shape=(28, 28, 1)),\n        Conv2D(32, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),  # Output: 14x14x32\n        \n        # Second Convolutional Block\n        Conv2D(64, (3, 3), activation='relu', padding='same'),\n        MaxPooling2D((2, 2)),  # Output: 7x7x64\n        \n        Flatten(),             # Output: 7*7*64 = 3136\n        Dense(128, activation='relu'),\n        Dropout(0.5),          # I use dropout to reduce overfitting\n        Dense(10, activation='softmax')  # 10 output classes (digits 0–9)\n    ])\n    \n    # I compile with Adam optimizer and sparse_categorical_crossentropy loss\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    return model\n\n# I instantiate the model.\nmodel = create_cnn_model()\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:12:18.000855Z","iopub.execute_input":"2025-06-06T04:12:18.001169Z","iopub.status.idle":"2025-06-06T04:12:18.071013Z","shell.execute_reply.started":"2025-06-06T04:12:18.001149Z","shell.execute_reply":"2025-06-06T04:12:18.070060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rationale\n\n- **Architecture Choices**:\n  1. Two convolutional blocks (Conv → ReLU → MaxPool) to extract spatial features at increasing depth (32→64 filters).\n  2. A fully connected layer with 128 units to combine extracted features.\n  3. Dropout (0.5) to reduce overfitting by randomly dropping connections during training.\n  4. Output layer with 10 units and softmax for multi-class classification.\n- I choose `sparse_categorical_crossentropy` because my labels are integer-encoded (0–9).  \n- Adam optimizer is widely used for image tasks—its adaptive learning rate helps the model converge faster.\n","metadata":{}},{"cell_type":"markdown","source":"### 4. Training the CNN","metadata":{}},{"cell_type":"code","source":"# I define an EarlyStopping callback to halt training if validation loss doesn't improve for 5 epochs.\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n    verbose=1\n)\n\n# I train for up to 30 epochs with batch size 128, using the validation set to monitor performance.\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=30,\n    batch_size=128,\n    callbacks=[early_stop],\n    verbose=2\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:13:32.077310Z","iopub.execute_input":"2025-06-06T04:13:32.077664Z","iopub.status.idle":"2025-06-06T04:19:26.721584Z","shell.execute_reply.started":"2025-06-06T04:13:32.077641Z","shell.execute_reply":"2025-06-06T04:19:26.720858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Rationale\n\n- **EarlyStopping**: I monitor `val_loss` and stop training once it stops improving for 5 consecutive epochs. This prevents overfitting and saves training time.\n- I chose 30 epochs as an upper bound, but EarlyStopping often finishes much earlier if the model converges.\n- A batch size of 128 balances GPU memory usage and gradient stability.\n","metadata":{}},{"cell_type":"markdown","source":"### 5. Training & Validation Metrics Visualization","metadata":{}},{"cell_type":"code","source":"# I extract the training and validation accuracy and loss history.\nacc      = history.history['accuracy']\nval_acc  = history.history['val_accuracy']\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\nepochs   = range(1, len(acc) + 1)\n\n# Plot Accuracy Curves\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, acc, label='Training Accuracy', marker='o')\nplt.plot(epochs, val_acc, label='Validation Accuracy', marker='o')\nplt.title('Accuracy over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.grid(True)\n\n# Plot Loss Curves\nplt.subplot(1, 2, 2)\nplt.plot(epochs, loss, label='Training Loss', marker='o')\nplt.plot(epochs, val_loss, label='Validation Loss', marker='o')\nplt.title('Loss over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:19:27.193961Z","iopub.execute_input":"2025-06-06T04:19:27.194255Z","iopub.status.idle":"2025-06-06T04:19:27.665859Z","shell.execute_reply.started":"2025-06-06T04:19:27.194234Z","shell.execute_reply":"2025-06-06T04:19:27.664959Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Rationale\n\n- By plotting training vs. validation accuracy and loss, I can check for overfitting:\n  - If validation accuracy plateaus or falls while training accuracy continues to rise, the model may be overfitting.\n  - If both curves improve and flatten out, the model is learning effectively.\n- Markers on the curves help pinpoint which epoch corresponds to which value.\n","metadata":{}},{"cell_type":"markdown","source":"### 6. Confusion Matrix on Validation Set","metadata":{}},{"cell_type":"code","source":"# I generate predictions on the validation set.\ny_val_pred_probs = model.predict(X_val, verbose=0)\ny_val_pred = np.argmax(y_val_pred_probs, axis=1)\n\n# I compute the confusion matrix.\ncm = confusion_matrix(y_val, y_val_pred)\n\n# I display it using sklearn's ConfusionMatrixDisplay.\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))\nplt.figure(figsize=(8, 8))\ndisp.plot(cmap='Blues', values_format='d', ax=plt.gca())\nplt.title('Confusion Matrix: CNN on Validation Set')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:19:29.155036Z","iopub.execute_input":"2025-06-06T04:19:29.155313Z","iopub.status.idle":"2025-06-06T04:19:30.480568Z","shell.execute_reply.started":"2025-06-06T04:19:29.155293Z","shell.execute_reply":"2025-06-06T04:19:30.479567Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Rationale\n\n- The confusion matrix shows which digits the model confuses most often.\n- A diagonal-dominant matrix (high counts along the diagonal) means most predictions are correct.\n- Off-diagonal entries highlight specific pairs (e.g., 5 vs. 3) that might require more augmentation or specialized attention.\n","metadata":{}},{"cell_type":"markdown","source":"### 7. Generating Submission","metadata":{}},{"cell_type":"code","source":"# I use the trained model to predict labels for the test set.\ntest_pred_probs = model.predict(X_test, verbose=0)\ntest_pred_labels = np.argmax(test_pred_probs, axis=1)\n\n# I prepare the submission DataFrame.\nsubmission = pd.DataFrame({\n    'ImageId': np.arange(1, len(test_pred_labels) + 1),\n    'Label':    test_pred_labels\n})\n\n# I save the submission to a CSV file.\nsubmission.to_csv('submission.csv', index=False)\n\n# I take a quick look at the first few rows of the submission.\nsubmission.head(15)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:43:46.588077Z","iopub.execute_input":"2025-06-06T04:43:46.588434Z","iopub.status.idle":"2025-06-06T04:43:53.224919Z","shell.execute_reply.started":"2025-06-06T04:43:46.588407Z","shell.execute_reply":"2025-06-06T04:43:53.224055Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Rationale\n\n- After confirming good validation performance, I predict on the competition’s test set (`test.csv`).\n- Kaggle expects a CSV with columns `ImageId,Label`, where `ImageId` runs from 1 to `N` in order.\n- Saving this file lets me submit directly on the Kaggle Digit Recognizer leaderboard.\n","metadata":{}},{"cell_type":"markdown","source":"### 8. Additional Data Visualization: Misclassified Examples","metadata":{}},{"cell_type":"code","source":"# I collect indices in the validation set where predictions were incorrect.\nmisclassified_idx = np.where(y_val_pred != y_val)[0]\n\n# I randomly select up to 6 misclassified examples to visualize.\nnp.random.seed(42)\nselected = np.random.choice(misclassified_idx, size=min(6, len(misclassified_idx)), replace=False)\n\nplt.figure(figsize=(12, 6))\nplt.suptitle('Random Misclassified Validation Examples', fontsize=16)\n\nfor i, idx in enumerate(selected):\n    ax = plt.subplot(2, 3, i + 1)\n    img = X_val[idx].reshape(28, 28)\n    true_label = y_val[idx]\n    pred_label = y_val_pred[idx]\n    \n    ax.imshow(img, cmap='gray')\n    ax.set_title(f\"True: {true_label}, Pred: {pred_label}\")\n    ax.axis('off')\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T04:19:42.449146Z","iopub.execute_input":"2025-06-06T04:19:42.449454Z","iopub.status.idle":"2025-06-06T04:19:43.259021Z","shell.execute_reply.started":"2025-06-06T04:19:42.449426Z","shell.execute_reply":"2025-06-06T04:19:43.258062Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Rationale\n\n- By plotting a few randomly selected misclassified images, I can inspect why the model got them wrong.\n- Some digits (e.g., poorly written “9” that looks like “4”) are naturally ambiguous; studying these helps refine data augmentation or labels in future iterations.\n","metadata":{}},{"cell_type":"markdown","source":"### 9. Summary\n\n- The CNN achieved high validation accuracy (>99%), indicating a strong fit on the MNIST-style data.\n- Confusion matrix analysis showed only a handful of misclassifications between visually similar digits (e.g., 8 vs. 3).\n- Potential improvements:\n  1. **Data Augmentation**: Introduce random rotations, shifts, or scale variations to make the model more robust.\n  2. **Ensemble Models**: Combine predictions from multiple architectures (e.g., a small ResNet or an autoencoder + classifier) to boost accuracy marginally.\n  3. **Hyperparameter Tuning**: Experiment with learning rates, batch sizes, or deeper networks using tools like Keras Tuner.\n  4. **Autoencoder Pretraining**: As in the reference code, pretrain digit-specific autoencoders to compute sample weights or to initialize encoder weights.\n","metadata":{}},{"cell_type":"markdown","source":"### 10. Last Thoughts\nFrom the confusion matrix, I observe that diagonal entries dominate, indicating very high per-digit accuracy with only a few off-diagonal errors. For instance, two ‘0’ examples are predicted as ‘6’, and five ‘8’s are misclassified as ‘9’. Statistically, this shows the model effectively differentiates digits but occasionally confuses visually similar shapes. The model summary indicates a compact CNN architecture: two convolutional blocks (32 and 64 filters) each followed by max-pooling, a flatten layer producing 3,136 features, a dense layer with 128 units and dropout, and a final dense layer with 10 outputs. With 421,642 parameters, the model balances expressiveness and generalization. The training curves reveal accuracy rising from 0.88 to 0.99 by epoch five, with validation accuracy closely tracking, suggesting minimal overfitting. Both training and validation losses decrease to about 0.04 and then plateau. Lastly, misclassified examples demonstrate ambiguous handwriting—like an ‘8’ that resembles ‘0’—highlighting that inherent noise in input data contributes to the few remaining errors.","metadata":{}}]}